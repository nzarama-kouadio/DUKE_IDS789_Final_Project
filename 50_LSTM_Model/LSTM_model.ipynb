{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Installing packages into ‘/home/codespace/R/x86_64-pc-linux-gnu-library/3.6’\n",
      "(as ‘lib’ is unspecified)\n",
      "\n",
      "Warning message:\n",
      "“dependencies ‘cpp11’, ‘slider’, ‘gtable’ are not available”\n",
      "also installing the dependencies ‘lava’, ‘prodlim’, ‘tfautograph’, ‘gargle’, ‘ids’, ‘timechange’, ‘systemfonts’, ‘textshaping’, ‘vroom’, ‘tzdb’, ‘rmarkdown’, ‘clock’, ‘ipred’, ‘htmlwidgets’, ‘tseries’, ‘tensorflow’, ‘tfruns’, ‘broom’, ‘dbplyr’, ‘dplyr’, ‘dtplyr’, ‘forcats’, ‘ggplot2’, ‘googledrive’, ‘googlesheets4’, ‘httr’, ‘lubridate’, ‘modelr’, ‘ragg’, ‘readr’, ‘readxl’, ‘reprex’, ‘rvest’, ‘tidyr’, ‘TTR’, ‘recipes’, ‘rsample’, ‘plotly’, ‘padr’, ‘forecast’, ‘tsfeatures’\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Install and load necessary libraries\n",
    "install.packages(c(\"keras\", \"tidyverse\", \"haven\", \"quantmod\", \"timetk\"))\n",
    "library(keras)\n",
    "library(tidyverse)\n",
    "library(haven)\n",
    "library(quantmod)\n",
    "library(timetk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Preprocess the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "data <- read_dta(\"50_LSTM_Model/data2.dta\")\n",
    "\n",
    "# Add year, quarter, and date columns\n",
    "data <- data %>%\n",
    "  mutate(\n",
    "    year = as.numeric(substr(time, 1, 4)),\n",
    "    q_str = substr(time, 5, 6),      # e.g. \"Q1\"\n",
    "    quarter = as.numeric(gsub(\"Q\", \"\", q_str)),\n",
    "    date = as.Date(paste(year, (quarter - 1)*3 + 1, \"01\", sep = \"-\"))\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add GDP/capita from FRED (Federal Reserve Economic Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Get U.S. GDP and population data\n",
    "getSymbols(\"GDP\", src = \"FRED\")  # Quarterly GDP (in billions)\n",
    "getSymbols(\"POP\", src = \"FRED\")  # Total U.S. population\n",
    "\n",
    "# Compute GDP per capita\n",
    "GDP_per_capita <- GDP / POP\n",
    "GDP_per_capita <- na.omit(GDP_per_capita)\n",
    "\n",
    "# Convert xts object to data frame\n",
    "GDP_per_capita_df <- data.frame(date = index(GDP_per_capita), GDP_per_capita = coredata(GDP_per_capita)) %>%\n",
    "  as_tibble()\n",
    "\n",
    "# Merge GDP per capita into the dataset\n",
    "data <- data %>%\n",
    "  left_join(GDP_per_capita_df, by = \"date\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Remove columns with excessive missing values\n",
    "na_counts <- colSums(is.na(data))\n",
    "columns_to_remove <- names(na_counts[na_counts > 20])\n",
    "data <- data %>% select(-all_of(columns_to_remove))\n",
    "\n",
    "# Handle remaining missing values\n",
    "data_nonNA <- na.omit(data)\n",
    "\n",
    "# Remove non-numerical columns\n",
    "data_nonNA <- data_nonNA %>% select(-time, -date)\n",
    "\n",
    "if (class(data_nonNA$deposits) == \"list\") {\n",
    "  data_nonNA$deposits <- as.numeric(unlist(data_nonNA$deposits))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Lagged Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Create lagged features for the 'deposits' column\n",
    "lagged_data <- tk_augment_lags(data_nonNA, .value = deposits, .lags = 1:10)\n",
    "\n",
    "# Remove rows with NAs (due to lags)\n",
    "lagged_data <- na.omit(lagged_data)\n",
    "\n",
    "# Define lagged columns if not already defined\n",
    "lagged_cols <- paste0(\"deposits_lag\", 1:10)\n",
    "\n",
    "# Convert lagged columns to numeric\n",
    "for (col in lagged_cols) {\n",
    "  if (class(lagged_data[[col]]) == \"list\") {\n",
    "    lagged_data[[col]] <- as.numeric(unlist(lagged_data[[col]]))\n",
    "  } else {\n",
    "    lagged_data[[col]] <- as.numeric(lagged_data[[col]])\n",
    "  }\n",
    "}\n",
    "\n",
    "# Ensure date columns exist in lagged_data\n",
    "lagged_data <- lagged_data %>% \n",
    "  mutate(date = as.Date(paste(year, (quarter - 1)*3 + 1, \"01\", sep=\"-\")))\n",
    "\n",
    "# Split into training and testing sets\n",
    "train_size <- floor(0.8 * nrow(lagged_data))\n",
    "train <- lagged_data[1:train_size, ]\n",
    "test <- lagged_data[(train_size + 1):nrow(lagged_data), ]\n",
    "\n",
    "# Prepare input data\n",
    "x_train <- train[, lagged_cols]\n",
    "x_test <- test[, lagged_cols]\n",
    "\n",
    "# Prepare target data\n",
    "y_train <- train$deposits\n",
    "y_test <- test$deposits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "str(train)\n",
    "str(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "summary(x_train)\n",
    "summary(y_train)\n",
    "\n",
    "dim(x_train)\n",
    "length(y_train)\n",
    "\n",
    "apply(x_train, 1, function(row) any(is.na(row)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Convert x_train and x_test to numeric matrices\n",
    "x_train_matrix <- as.matrix(x_train)\n",
    "x_test_matrix <- as.matrix(x_test)\n",
    "# Ensure all elements are numeric\n",
    "x_train_matrix <- apply(x_train_matrix, 2, as.numeric)\n",
    "x_test_matrix <- apply(x_test_matrix, 2, as.numeric)\n",
    "\n",
    "# Ensure y_train and y_test are numeric vectors\n",
    "y_train <- as.numeric(train$deposits)\n",
    "y_test <- as.numeric(test$deposits)\n",
    "\n",
    "# Convert to numeric matrices\n",
    "x_train_matrix <- as.matrix(x_train)\n",
    "x_test_matrix <- as.matrix(x_test)\n",
    "\n",
    "# Scale training data column-wise\n",
    "x_train_scaled <- scale(x_train_matrix)  # This scales each column by its own mean and sd\n",
    "x_mean <- attr(x_train_scaled, \"scaled:center\")\n",
    "x_sd <- attr(x_train_scaled, \"scaled:scale\")\n",
    "\n",
    "# Scale test data using training set statistics\n",
    "x_test_scaled <- scale(x_test_matrix, center = x_mean, scale = x_sd)\n",
    "\n",
    "# Remove clipping or only apply it if absolutely necessary\n",
    "# x_train_scaled <- pmin(pmax(x_train_scaled, -3), 3)\n",
    "# x_test_scaled <- pmin(pmax(x_test_scaled, -3), 3)\n",
    "\n",
    "# Normalize y_train and y_test separately\n",
    "y_mean <- mean(y_train)\n",
    "y_sd <- sd(y_train)\n",
    "y_train_normalized <- (y_train - y_mean) / y_sd\n",
    "y_test_normalized <- (y_test - y_mean) / y_sd\n",
    "\n",
    "# Confirm normalization\n",
    "summary(as.vector(x_train))\n",
    "summary(as.vector(x_test))\n",
    "summary(y_train)\n",
    "summary(y_test)\n",
    "\n",
    "# Reshape input data to 3D arrays for LSTM [samples, time steps, features]\n",
    "x_train_array <- array(\n",
    "  x_train_scaled,\n",
    "  dim = c(nrow(x_train_scaled), ncol(x_train_scaled), 1)\n",
    ")\n",
    "x_test_array <- array(\n",
    "  x_test_scaled,\n",
    "  dim = c(nrow(x_test_scaled), ncol(x_test_scaled), 1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Benefits of Normalization:\n",
    "Reduces the MSE to a manageable scale, allowing for better interpretation.\n",
    "Improves model convergence by ensuring that inputs and outputs are within similar ranges.\n",
    "\n",
    "## Define and Train LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "```{r}\n",
    "# Define LSTM model with non-linear activation functions\n",
    "model <- keras_model_sequential() %>%\n",
    "  # Add LSTM layers with tanh activation (default for LSTMs)\n",
    "  layer_lstm(units = 64, input_shape = c(ncol(x_train_scaled), 1), \n",
    "             activation = \"tanh\", return_sequences = TRUE) %>%\n",
    "  layer_dropout(rate = 0.2) %>%\n",
    "  layer_lstm(units = 32, activation = \"tanh\", return_sequences = FALSE) %>%\n",
    "  layer_dropout(rate = 0.2) %>%\n",
    "  # Add a dense layer with a non-linear activation (e.g., ReLU)\n",
    "  layer_dense(units = 16, activation = \"relu\") %>%\n",
    "  # Final output layer remains linear (no activation) to predict continuous values\n",
    "  layer_dense(units = 1)\n",
    "\n",
    "# Compile the model\n",
    "model %>% compile(\n",
    "  optimizer = optimizer_adam(learning_rate = 0.001),  # Use Adam optimizer\n",
    "  loss = \"mse\",                                      # Minimize Mean Squared Error\n",
    "  metrics = c(\"mae\")                                 # Evaluate Mean Absolute Error\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "history <- model %>% fit(\n",
    "  x_train_array, y_train_normalized,                 # Input data\n",
    "  epochs = 50,                                       # Number of training epochs\n",
    "  batch_size = 32,                                   # Batch size\n",
    "  validation_split = 0.2                             # Use 20% of training data for validation\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "plot(history) +\n",
    "  labs(\n",
    "    title = \"Training and Validation Metrics Over Epochs\",\n",
    "    x = \"Epoch\",\n",
    "    y = \"Metrics\"\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Predictions and Calculate MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "predictions <- model %>% predict(x_test_array)\n",
    "\n",
    "# Rescale predictions back to original scale\n",
    "predictions_rescaled <- predictions * y_sd + y_mean\n",
    "\n",
    "# Rescale y_test back to original scale\n",
    "y_test_rescaled <- y_test_normalized * y_sd + y_mean\n",
    "\n",
    "# Align lengths\n",
    "common_length <- min(length(y_test_rescaled), length(predictions_rescaled))\n",
    "y_test_rescaled <- y_test_rescaled[1:common_length]\n",
    "predictions_rescaled <- predictions_rescaled[1:common_length]\n",
    "\n",
    "# Calculate Mean Squared Error\n",
    "mse <- mean((y_test_rescaled - predictions_rescaled)^2)\n",
    "print(paste(\"Mean Squared Error:\", mse))\n",
    "\n",
    "# Calculate RMSE\n",
    "rmse <- sqrt(mse)\n",
    "\n",
    "# Calculate mean of the actual values\n",
    "mean_actual <- mean(y_test_rescaled)\n",
    "\n",
    "# Calculate relative error\n",
    "relative_error <- (rmse / mean_actual) * 100\n",
    "print(paste(\"RMSE:\", rmse))\n",
    "print(paste(\"Mean Actual Value:\", mean_actual))\n",
    "print(paste(\"Relative Error (%):\", relative_error))\n",
    "\n",
    "# Calculate the model's MAE (numerator)\n",
    "mae <- mean(abs(y_test_rescaled - predictions_rescaled))\n",
    "# Calculate the naive MAE (denominator)\n",
    "naive_mae <- mean(abs(diff(y_test_rescaled)))\n",
    "# Calculate MASE\n",
    "mase <- mae / naive_mae\n",
    "print(paste(\"Mean Absolute Scaled Error (MASE):\", mase))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Define the MASE function\n",
    "mase <- function(y_train, y_test, y_preds) {\n",
    "  n <- length(y_train)\n",
    "  m <- length(y_test)\n",
    "  \n",
    "  # Calculate the denominator (scaled error from y_train)\n",
    "  denom <- 0\n",
    "  for (i in 1:(n - m)) {\n",
    "    # Compute the mean absolute difference for the m-length window\n",
    "    denom <- denom + mean(abs(y_train[(i + 1):(i + m)] - rep(y_train[i], m)))\n",
    "  }\n",
    "  denom <- denom / (n - m)\n",
    "  \n",
    "  # Calculate the numerator (mean absolute error for predictions)\n",
    "  num <- mean(abs(y_test - y_preds))\n",
    "  \n",
    "  # Return the MASE\n",
    "  return(num / denom)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "y_train_rescaled <- y_train_normalized * y_sd + y_mean\n",
    "\n",
    "mase(y_train_rescaled, y_test_rescaled, predictions_rescaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Mean Squared Error (MSE): 346,875,766.038345\n",
    "MSE measures the average squared difference between the predicted and actual values. Since the target variable (deposits) has large values (mean of ~320,275), a high MSE is expected due to the squaring of errors. MSE is useful for optimization during training but can be less interpretable in isolation due to scaling.\n",
    "\n",
    "### Root Mean Squared Error (RMSE): 18,624.60\n",
    "RMSE is the square root of MSE, providing an error metric in the same units as the target variable. An RMSE of ~18,625 against a mean deposit value of ~320,275 represents the average magnitude of prediction errors. This suggests that, on average, the model's predictions are off by approximately 18,625 units of deposits.\n",
    "\n",
    "### Mean Actual Value: 320,275.44\n",
    "This is the average of the actual deposits values in your test set. It provides context for interpreting RMSE and relative error.\n",
    "\n",
    "### Relative Error (%): 5.815%\n",
    "Relative Error is calculated as (RMSE / Mean Actual Value) * 100%.\n",
    "The model's predictions deviate from the actual values by approximately 5.82% on average.\n",
    "In many financial forecasting contexts, a relative error below 10% is considered acceptable.\n",
    "\n",
    "### Mean Absolute Scaled Error (MASE): 0.3368904\n",
    "A Mean Absolute Scaled Error (MASE) of 0.3368904 indicates that the model’s predictions are significantly better than a naïve baseline, with prediction errors being only about 34% as large as those of the baseline. This result confirms that your LSTM model is effectively capturing patterns in the data and leveraging temporal dependencies, making it a suitable and accurate choice for the task.\n",
    "\n",
    "\n",
    "### Recommendations\n",
    "To decide whether to use this model, consider the following factors:\n",
    "\n",
    "Business Context: If the goal is to achieve predictions within a 5.40% error margin and absolute errors around 17,309 units are acceptable for decision-making, this model might suffice. However, the high MASE indicates that there is room for improvement, especially if a naïve baseline could perform nearly five times better.\n",
    "\n",
    "Potential Refinements: To address the high MASE, explore adding lagged features, seasonal indicators, or external factors (e.g., interest rates, economic indicators) that might affect bank deposit levels. Experimenting with model architecture (e.g., more LSTM layers, bidirectional LSTMs) or hyperparameters could also help capture temporal patterns more effectively.\n",
    "\n",
    "Comparison with Simpler Models: Since the naïve baseline outperforms this model (as indicated by the MASE), consider trying simpler models like ARIMA, exponential smoothing, or linear regression with lagged variables. These models are easier to implement and may perform better on data with strong autocorrelation.\n",
    "\n",
    "### Actual vs Predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Ensure the 'date' column is in Date format\n",
    "data$date <- as.Date(data$date)\n",
    "\n",
    "# Plot deposits over time\n",
    "ggplot(data, aes(x = date, y = deposits)) +\n",
    "  geom_line(color = \"blue\", size = 1) +  # Line plot for deposits\n",
    "  labs(\n",
    "    title = \"Deposits Over Time\",\n",
    "    x = \"Date\",\n",
    "    y = \"Deposits (in $)\"\n",
    "  ) +\n",
    "  theme_classic() +\n",
    "  theme(\n",
    "    plot.title = element_text(hjust = 0.5, size = 14, face = \"bold\"),\n",
    "    axis.title.x = element_text(size = 12),\n",
    "    axis.title.y = element_text(size = 12)\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "future_steps <- 10\n",
    "\n",
    "# Get the last known set of lagged values from the test set\n",
    "last_row <- tail(lagged_data, 1)\n",
    "current_lagged_values <- as.numeric(last_row[lagged_cols])\n",
    "\n",
    "# Ensure numeric and scale correctly\n",
    "current_lagged_scaled <- (current_lagged_values - x_mean) / x_sd\n",
    "\n",
    "# Initialize vector for future predictions\n",
    "future_predictions_rescaled <- numeric(future_steps)\n",
    "\n",
    "# Iteratively predict future steps\n",
    "for (i in 1:future_steps) {\n",
    "  # Reshape for LSTM input: [samples, timesteps, features]\n",
    "  current_lagged_array <- array(current_lagged_scaled, \n",
    "                                dim = c(1, length(current_lagged_values), 1))\n",
    "  \n",
    "  # Predict next value\n",
    "  pred <- model %>% predict(current_lagged_array)\n",
    "  \n",
    "  # Rescale prediction\n",
    "  pred_rescaled <- pred * y_sd + y_mean\n",
    "  \n",
    "  # Store prediction\n",
    "  future_predictions_rescaled[i] <- pred_rescaled\n",
    "  \n",
    "  # Update lagged values: drop the oldest and add the new prediction\n",
    "  current_lagged_values <- c(current_lagged_values[-1], pred_rescaled)\n",
    "  \n",
    "  # Re-scale for next iteration\n",
    "  current_lagged_scaled <- (current_lagged_values - x_mean) / x_sd\n",
    "}\n",
    "\n",
    "# Use the existing test$date for actual values\n",
    "test_dates <- test$date\n",
    "\n",
    "# Generate future quarterly dates starting from the last test date\n",
    "# Assuming quarterly data, we add 3 months per future step\n",
    "last_test_date <- max(test_dates)\n",
    "future_dates <- seq.Date(from = last_test_date %m+% months(3), \n",
    "                         by = \"3 months\", \n",
    "                         length.out = future_steps)\n",
    "\n",
    "# Combine actual and predicted values\n",
    "actual_values <- c(y_test_rescaled, rep(NA, future_steps))\n",
    "predicted_values <- c(predictions_rescaled, future_predictions_rescaled)\n",
    "\n",
    "comparison_data <- data.frame(\n",
    "  Date = c(test_dates, future_dates),\n",
    "  Actual = actual_values,\n",
    "  Predicted = predicted_values\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "comparison_data$Date <- as.Date(comparison_data$Date)\n",
    "\n",
    "# Plot predicted deposits over time\n",
    "ggplot(comparison_data, aes(x = Date, y = Predicted)) +\n",
    "  geom_line(color = \"red\", size = 1) +  # Line plot for predicted deposits\n",
    "  labs(\n",
    "    title = \"Predicted Deposits Over Time\",\n",
    "    x = \"Date\",\n",
    "    y = \"Predicted Deposits (in $)\"\n",
    "  ) +\n",
    "  theme_classic() +\n",
    "  theme(\n",
    "    plot.title = element_text(hjust = 0.5, size = 14, face = \"bold\"),\n",
    "    axis.title.x = element_text(size = 12),\n",
    "    axis.title.y = element_text(size = 12)\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Step 4: Plot the actual and predicted values\n",
    "ggplot(comparison_data, aes(x = Date)) +\n",
    "  geom_line(aes(y = Actual, color = \"Actual\")) +\n",
    "  geom_line(aes(y = Predicted, color = \"Predicted\")) +\n",
    "  labs(\n",
    "    title = \"Actual vs Predicted Deposit Levels\",\n",
    "    x = \"Date\",\n",
    "    y = \"Deposit Level\",\n",
    "    color = \"Legend\"\n",
    "  ) +\n",
    "  theme_minimal()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Add uncertainty intervals to the comparison_data if not already present\n",
    "comparison_data <- comparison_data %>%\n",
    "  mutate(\n",
    "    yhat_lower = Predicted - 1.96 * sd(Predicted, na.rm = TRUE), # Replace with appropriate logic if needed\n",
    "    yhat_upper = Predicted + 1.96 * sd(Predicted, na.rm = TRUE)\n",
    "  )\n",
    "\n",
    "# Ensure Date is in the correct format\n",
    "comparison_data$Date <- as.Date(comparison_data$Date)\n",
    "\n",
    "# Create the combined plot\n",
    "ggplot() +\n",
    "  # Add historical data points (Actual values)\n",
    "  geom_point(data = comparison_data, aes(x = Date, y = Actual, color = \"Historical Data\"), size = 1.5, alpha = 0.8) +\n",
    "  # Add the predicted trend line\n",
    "  geom_line(data = comparison_data, aes(x = Date, y = Predicted, color = \"Predicted Trend\"), size = 1.5) +\n",
    "  # Add uncertainty intervals for predictions\n",
    "  geom_ribbon(\n",
    "    data = comparison_data,\n",
    "    aes(x = Date, ymin = yhat_lower, ymax = yhat_upper, fill = \"Uncertainty Interval\"),\n",
    "    alpha = 0.2\n",
    "  ) +\n",
    "  # Add labels and a title\n",
    "  labs(\n",
    "    title = \"Historical Data and Prediction with LSTM\",\n",
    "    x = \"Date\",\n",
    "    y = \"Deposits (in $)\",\n",
    "    color = \"Legend\",\n",
    "    fill = \"Legend\"\n",
    "  ) +\n",
    "  # Customize colors for the legend\n",
    "  scale_color_manual(\n",
    "    values = c(\"Historical Data\" = \"black\", \"Predicted Trend\" = \"red\")\n",
    "  ) +\n",
    "  scale_fill_manual(\n",
    "    values = c(\"Uncertainty Interval\" = \"blue\")\n",
    "  ) +\n",
    "  # Add a classic theme\n",
    "  theme_classic() +\n",
    "  theme(\n",
    "    plot.title = element_text(hjust = 0.5, size = 14, face = \"bold\"),\n",
    "    axis.title.x = element_text(size = 12),\n",
    "    axis.title.y = element_text(size = 12),\n",
    "    legend.title = element_text(size = 12, face = \"bold\"),\n",
    "    legend.text = element_text(size = 10)\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Select only numeric columns\n",
    "numeric_data <- data %>% select(where(is.numeric))\n",
    "\n",
    "# Ensure 'deposits' is included in the numeric data\n",
    "if (!\"deposits\" %in% colnames(numeric_data)) {\n",
    "  stop(\"Variable 'deposits' is not numeric or missing from the dataset.\")\n",
    "}\n",
    "\n",
    "# Remove the 'deposits' column from predictors\n",
    "predictors <- numeric_data %>% select(-deposits)\n",
    "\n",
    "# Perform correlation analysis\n",
    "cor_analysis <- cor(predictors, numeric_data$deposits, use = \"complete.obs\")\n",
    "\n",
    "# Sort and print results\n",
    "cor_analysis <- sort(cor_analysis, decreasing = TRUE)\n",
    "print(cor_analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Calculate correlations for numeric columns\n",
    "numeric_data <- data %>% select(where(is.numeric))\n",
    "\n",
    "# Ensure 'deposits' is numeric\n",
    "if (!\"deposits\" %in% colnames(numeric_data)) {\n",
    "  stop(\"Variable 'deposits' is not numeric or missing from the dataset.\")\n",
    "}\n",
    "\n",
    "# Remove the 'deposits' column from predictors\n",
    "predictors <- numeric_data %>% select(-deposits)\n",
    "\n",
    "# Correlation matrix\n",
    "cor_matrix <- cor(predictors, numeric_data$deposits, use = \"complete.obs\")\n",
    "\n",
    "# Convert to data frame for visualization\n",
    "cor_df <- data.frame(Variable = rownames(cor_matrix), Correlation = cor_matrix[, 1])\n",
    "cor_df <- cor_df %>% arrange(desc(Correlation))\n",
    "\n",
    "# Select top 20 variables by absolute correlation\n",
    "top_20_cor <- cor_df %>% slice(1:20)\n",
    "\n",
    "# Visualize the top 15 correlations\n",
    "ggplot(top_20_cor, aes(x = reorder(Variable, Correlation), y = Correlation)) +\n",
    "  geom_bar(stat = \"identity\", fill = \"steelblue\") +\n",
    "  coord_flip() +\n",
    "  labs(\n",
    "    title = \"Top 20 Variables Correlated with Deposits\",\n",
    "    x = \"Variables\",\n",
    "    y = \"Correlation\"\n",
    "  ) +\n",
    "  theme_minimal() +\n",
    "  theme(\n",
    "    plot.title = element_text(hjust = 0.5, size = 12, face = \"bold\"),\n",
    "    axis.title = element_text(size = 12),\n",
    "    axis.text.y = element_text(size = 8),\n",
    "    axis.text = element_text(size = 10)\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "#library(car)\n",
    "# Select top 20 variables (based on correlation from your plot)\n",
    "top_variables <- c(\n",
    "  \"goodwill\", \"loans_constr\", \"loans_cc\", \"loans_hel\", \"deposits_noninterest\", \n",
    "  \"securities\", \"reserve_losses\", \"sec_held_mat\", \"inc_comprehensive\", \"deposits_foreign\",\n",
    "  \"bank_eq_cap\", \"loan_commit\", \"loans_multi\", \"stock_common\", \"surplus\",\n",
    "  \"deposits_domestic\", \"loans_nonres\", \"assets_earning\", \"stock_pref\", \"fv_mtg_serv\"\n",
    ")\n",
    "\n",
    "# Subset data\n",
    "data_top <- data %>% select(all_of(top_variables))\n",
    "data_top <- cbind(deposits = data$deposits, data_top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Calculate correlation matrix for predictors\n",
    "cor_matrix <- cor(data_top %>% select(-deposits), use = \"complete.obs\")\n",
    "\n",
    "# Find pairs of highly correlated variables (e.g., correlation > 0.9)\n",
    "high_corr_pairs <- which(abs(cor_matrix) > 0.9, arr.ind = TRUE)\n",
    "\n",
    "# Exclude self-correlations\n",
    "high_corr_pairs <- high_corr_pairs[high_corr_pairs[, 1] != high_corr_pairs[, 2], ]\n",
    "\n",
    "# Print high-correlation pairs for inspection\n",
    "if (nrow(high_corr_pairs) > 0) {\n",
    "  print(high_corr_pairs)\n",
    "} else {\n",
    "  print(\"No highly correlated variables found.\")\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "data2 <- read_dta(\"data2.dta\")\n",
    "\n",
    "data2 <- data2 %>%\n",
    "  mutate(\n",
    "    year = as.numeric(substr(time, 1, 4)),\n",
    "    q_str = substr(time, 5, 6),      # e.g. \"Q1\"\n",
    "    quarter = as.numeric(gsub(\"Q\", \"\", q_str)),\n",
    "    date = as.Date(paste(year, (quarter - 1)*3 + 1, \"01\", sep = \"-\"))\n",
    "  )\n",
    "\n",
    "selected_vars <- c(\"date\", \"deposits\", \"goodwill\", \"loans_constr\", \n",
    "                   \"deposits_noninterest\", \"securities\", \"bank_eq_cap\", \n",
    "                   \"assets_earning\", \"surplus\")\n",
    "\n",
    "data2 <- data2 %>%\n",
    "  select(all_of(selected_vars))\n",
    "\n",
    "# Remove columns with excessive missing values\n",
    "na_counts <- colSums(is.na(data2))\n",
    "columns_to_remove <- names(na_counts[na_counts > 20])\n",
    "data2 <- data2 %>%\n",
    "  select(-all_of(columns_to_remove))\n",
    "\n",
    "# Handle remaining missing values\n",
    "data2 <- na.omit(data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Create lagged features for the variables of interest\n",
    "lag_vars <- c(\"deposits\", \"goodwill\", \"loans_constr\", \"deposits_noninterest\", \n",
    "              \"securities\", \"bank_eq_cap\", \"assets_earning\", \"surplus\")\n",
    "for (var in lag_vars) {\n",
    "  data2 <- data2 %>%\n",
    "    tk_augment_lags(.value = !!sym(var), .lags = 1:10)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Remove rows with NAs (caused by lagging)\n",
    "data2 <- na.omit(data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Extract lagged column names\n",
    "lagged_cols <- names(data2)[grepl(\"_lag[0-9]+\", names(data2))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Extract features and target\n",
    "x_data <- data2[, lagged_cols]\n",
    "y_data <- data2$deposits\n",
    "\n",
    "# Split into training and testing sets\n",
    "train_size <- floor(0.8 * nrow(data2))\n",
    "\n",
    "x_train <- x_data[1:train_size, ]\n",
    "x_test <- x_data[(train_size + 1):nrow(x_data), ]\n",
    "\n",
    "y_train <- y_data[1:train_size]\n",
    "y_test <- y_data[(train_size + 1):length(y_data)]\n",
    "\n",
    "# Confirm the dimensions of the training and testing sets\n",
    "print(dim(x_train))  # Check rows and columns of x_train\n",
    "print(length(y_train))  # Check rows of y_train\n",
    "print(dim(x_test))  # Check rows and columns of x_test\n",
    "print(length(y_test))  # Check rows of y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Align rows for training and testing sets\n",
    "min_train_rows <- min(nrow(x_train), length(y_train))\n",
    "x_train <- x_train[1:min_train_rows, ]\n",
    "y_train <- y_train[1:min_train_rows]\n",
    "\n",
    "min_test_rows <- min(nrow(x_test), length(y_test))\n",
    "x_test <- x_test[1:min_test_rows, ]\n",
    "y_test <- y_test[1:min_test_rows]\n",
    "\n",
    "# Confirm dimensions\n",
    "print(dim(x_train))  # Should match length(y_train)\n",
    "print(length(y_train))\n",
    "print(dim(x_test))  # Should match length(y_test)\n",
    "print(length(y_test))\n",
    "\n",
    "# Convert x_train and x_test to numeric matrices\n",
    "x_train_matrix <- as.matrix(x_train)\n",
    "x_test_matrix <- as.matrix(x_test)\n",
    "\n",
    "# Scale training data column-wise\n",
    "x_train_scaled <- scale(x_train_matrix)  # This scales each column by its own mean and sd\n",
    "x_mean <- attr(x_train_scaled, \"scaled:center\")\n",
    "x_sd <- attr(x_train_scaled, \"scaled:scale\")\n",
    "\n",
    "# Scale testing data using training set statistics\n",
    "x_test_scaled <- scale(x_test_matrix, center = x_mean, scale = x_sd)\n",
    "\n",
    "# Normalize y_train and y_test\n",
    "y_mean <- mean(y_train)\n",
    "y_sd <- sd(y_train)\n",
    "y_train_normalized <- (y_train - y_mean) / y_sd\n",
    "y_test_normalized <- (y_test - y_mean) / y_sd\n",
    "\n",
    "# Confirm scaling\n",
    "summary(x_train_scaled)\n",
    "summary(x_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Reshape input data to 3D arrays for LSTM [samples, time steps, features]\n",
    "x_train_array <- array(\n",
    "  x_train_scaled,\n",
    "  dim = c(nrow(x_train_scaled), ncol(x_train_scaled), 1)\n",
    ")\n",
    "x_test_array <- array(\n",
    "  x_test_scaled,\n",
    "  dim = c(nrow(x_test_scaled), ncol(x_test_scaled), 1)\n",
    ")\n",
    "\n",
    "# Confirm dimensions\n",
    "print(dim(x_train_array))\n",
    "print(dim(x_test_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "#library(keras)\n",
    "\n",
    "# Redefine the LSTM model\n",
    "model <- keras_model_sequential() %>%\n",
    "  layer_lstm(units = 128, input_shape = c(ncol(x_train_array), 1), \n",
    "             activation = \"tanh\", return_sequences = TRUE, \n",
    "             kernel_regularizer = regularizer_l2(0.001)) %>%\n",
    "  layer_dropout(rate = 0.3) %>%\n",
    "  layer_lstm(units = 64, activation = \"tanh\", return_sequences = FALSE, \n",
    "             kernel_regularizer = regularizer_l2(0.001)) %>%\n",
    "  layer_dropout(rate = 0.3) %>%\n",
    "  layer_dense(units = 32, activation = \"relu\") %>%\n",
    "  layer_dense(units = 1)\n",
    "\n",
    "# Compile the model\n",
    "model %>% compile(\n",
    "  optimizer = optimizer_adam(learning_rate = 0.001),\n",
    "  loss = \"mse\",\n",
    "  metrics = c(\"mae\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history <- model %>% fit(\n",
    "  x_train_array, y_train_normalized,\n",
    "  epochs = 50,\n",
    "  batch_size = 32,\n",
    "  validation_split = 0.2\n",
    ")\n",
    "\n",
    "summary(history)\n",
    "plot(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Predict\n",
    "y_pred_normalized <- model %>% predict(x_test_array)\n",
    "\n",
    "# Reverse normalization\n",
    "y_pred <- (y_pred_normalized * y_sd) + y_mean\n",
    "\n",
    "# Compare predictions with actual values\n",
    "results <- data.frame(\n",
    "  Actual = y_test,\n",
    "  Predicted = as.vector(y_pred)\n",
    ")\n",
    "print(head(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "residuals <- y_test - y_pred\n",
    "plot(residuals, main = \"Residuals\", ylab = \"Residuals\", xlab = \"Index\")\n",
    "abline(h = 0, col = \"red\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Preprocess data from 1984 to 2024\n",
    "data_1984_2024 <- data %>%\n",
    "  select(date, deposits, goodwill, loans_constr, deposits_noninterest, \n",
    "         securities, bank_eq_cap, assets_earning, surplus)\n",
    "\n",
    "lag_vars <- c(\"deposits\", \"goodwill\", \"loans_constr\", \"deposits_noninterest\", \n",
    "              \"securities\", \"bank_eq_cap\", \"assets_earning\", \"surplus\")\n",
    "\n",
    "for (var in lag_vars) {\n",
    "  data_1984_2024 <- data_1984_2024 %>%\n",
    "    tk_augment_lags(.value = !!sym(var), .lags = 1:10)\n",
    "}\n",
    "\n",
    "data_1984_2024 <- na.omit(data_1984_2024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "x_1984_2024 <- scale(as.matrix(data_1984_2024[, lagged_cols]), \n",
    "                     center = x_mean, scale = x_sd)\n",
    "\n",
    "x_1984_2024_array <- array(\n",
    "  x_1984_2024,\n",
    "  dim = c(nrow(x_1984_2024), ncol(x_1984_2024), 1)\n",
    ")\n",
    "\n",
    "x_1984_2024_array <- array(\n",
    "  x_1984_2024,\n",
    "  dim = c(nrow(x_1984_2024), ncol(x_1984_2024), 1)\n",
    ")\n",
    "\n",
    "y_pred_normalized <- model %>% predict(x_1984_2024_array)\n",
    "\n",
    "# Reverse normalization\n",
    "y_pred <- (y_pred_normalized * y_sd) + y_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "results_1984_2024 <- data.frame(\n",
    "  Date = data_1984_2024$date,\n",
    "  Actual = data_1984_2024$deposits,\n",
    "  Predicted = as.vector(y_pred)\n",
    ")\n",
    "print(head(results_1984_2024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "ggplot(results_1984_2024, aes(x = Date)) +\n",
    "  geom_line(aes(y = Actual, color = \"Actual\"), size = 1) +\n",
    "  geom_line(aes(y = Predicted, color = \"Predicted\"), size = 1, linetype = \"dashed\") +\n",
    "  labs(\n",
    "    title = \"Deposits: Predicted vs. Actual (2000–2024)\",\n",
    "    x = \"Date\",\n",
    "    y = \"Deposits\",\n",
    "    color = \"Legend\"\n",
    "  ) +\n",
    "  theme_minimal() +\n",
    "  theme(\n",
    "    legend.position = \"top\",\n",
    "    legend.title = element_blank(),\n",
    "    text = element_text(size = 12)\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Narrow the uncertainty interval to reflect a smaller range\n",
    "comparison_data <- comparison_data %>%\n",
    "  mutate(\n",
    "    yhat_lower = Predicted - 0.15 * sd(Predicted, na.rm = TRUE), # Adjusted factor for narrower interval\n",
    "    yhat_upper = Predicted + 0.15 * sd(Predicted, na.rm = TRUE)\n",
    "  )\n",
    "\n",
    "# Ensure Date is in the correct format\n",
    "comparison_data$Date <- as.Date(comparison_data$Date)\n",
    "\n",
    "# Create a larger, zoomed-in plot\n",
    "ggplot() +\n",
    "  # Add historical data points (Actual values)\n",
    "  geom_point(data = comparison_data, aes(x = Date, y = Actual, color = \"Historical Data\"), size = 0.8, alpha = 0.8) +\n",
    "  # Add the predicted trend line\n",
    "  geom_line(data = comparison_data, aes(x = Date, y = Predicted, color = \"Predicted Trend\"), size = 0.8) +\n",
    "  # Add narrower uncertainty intervals for predictions\n",
    "  geom_ribbon(\n",
    "    data = comparison_data,\n",
    "    aes(x = Date, ymin = yhat_lower, ymax = yhat_upper, fill = \"Uncertainty Interval\"),\n",
    "    alpha = 0.2\n",
    "  ) +\n",
    "  # Add labels and a title\n",
    "  labs(\n",
    "    title = \"Deposits: Actual vs. Predicted\",\n",
    "    subtitle = \"0.15 SD Narroed Prediction Intervals\",\n",
    "    x = \"Date\",\n",
    "    y = \"Deposits (in Million $)\",\n",
    "    color = \"Legend\",\n",
    "    fill = \"Legend\"\n",
    "  ) +\n",
    "  # Customize colors for the legend\n",
    "  scale_color_manual(\n",
    "    values = c(\"Historical Data\" = \"black\", \"Predicted Trend\" = \"red\")\n",
    "  ) +\n",
    "  scale_fill_manual(\n",
    "    values = c(\"Uncertainty Interval\" = \"blue\")\n",
    "  ) +\n",
    "  # Add a classic theme with larger graphical area\n",
    "  theme_classic() +\n",
    "  theme(\n",
    "    plot.title = element_text(hjust = 0.5, size = 16, face = \"bold\"),\n",
    "    plot.subtitle = element_text(hjust = 0.5, size = 12),\n",
    "    axis.title.x = element_text(size = 14),\n",
    "    axis.title.y = element_text(size = 14),\n",
    "    legend.title = element_text(size = 14, face = \"bold\"),\n",
    "    legend.text = element_text(size = 12),\n",
    "    axis.text = element_text(size = 12),\n",
    "    plot.margin = margin(10, 10, 10, 10, \"pt\")  # Increase plot margin\n",
    "  ) +\n",
    "  # Adjust y-axis range to focus more on the trend\n",
    "  coord_cartesian(ylim = c(min(comparison_data$Actual, na.rm = TRUE), max(comparison_data$Predicted, na.rm = TRUE)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
