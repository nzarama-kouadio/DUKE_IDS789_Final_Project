{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Load Libaries and Import necessary dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# https://otexts.com/fpp3/prophet.html\n",
    "# Load required libraries. Make sure to install them too\n",
    "library(readxl)\n",
    "library(dplyr)\n",
    "library(Rcpp)\n",
    "library(ggplot2)\n",
    "library(lubridate)\n",
    "library(forecast)\n",
    "library(prophet)\n",
    "library(Metrics)\n",
    "library(zoo) # use for date formatting \n",
    "library(stats) # use for additive\n",
    "library(caret)\n",
    "library(glmnet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Extract deposit_levels to use it in this file\n",
    "deposit_levels <- readRDS(\"/workspaces/DUKE_MIDS_QFC_Final_Project/data_cleaning/deposit_levels.rds\") # Load the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Extract regressors_data to use it in this file\n",
    "regressors_data <- readRDS(\"/workspaces/DUKE_MIDS_QFC_Final_Project/data_cleaning/regressors_data.rds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Data Exploration for Prophet Assumption"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Purpose**: Prophet requires data to be regular (no gaps in time periods) and free of missing values to build accurate forecasts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "sum(is.na(deposit_levels))  # no missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The time series has regular quaterly intervals and no missing values so the data regurlarity expectation is satified. \n",
    "\n",
    "- It also has decades worth of data so the \"sufficient historical data\" expectation is satisfied. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Purpose**: Prophet models require a clear understanding of the trend to accurately forecast future values. \n",
    "\n",
    "- **Next Steps**: Evaluate whether the data exhibits a linear or logistic trend.\n",
    "\n",
    "- **Question** Does the choice of a linear trend or logistic trend affect Prophet's predictions?\n",
    "\n",
    "- **Answer** : Yes, it absolutely affects the predictions. Here’s how:\n",
    "\n",
    "    - **Linear Trend**: Assumes that growth continues indefinitely at a constant rate.\n",
    "        - Future predictions will show a strong upward trend, even if the data is slowing down.\n",
    "        - Example: If you fit a linear trend to deposit levels, Prophet will extrapolate a steadily increasing deposit level far into the future.\n",
    "\n",
    "    - **Logistic Trend**: Assumes growth slows as it approaches a saturation point. \n",
    "        - Future predictions will show a more tamed upward trend, leveling off over time.\n",
    "        - Example: If deposits are approaching a plateau, a logistic trend will reflect that in future predictions, ensuring the model doesn't over-predict growth.\n",
    "\n",
    "- **Key Insight**:If you fit Prophet with the wrong trend type (e.g., linear when the data shows signs of saturation), your forecasts will misrepresent the future. Context is critical here: If you know from domain expertise or external analysis that growth will saturate, using a logistic trend will give more realistic predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. Formatting the time as a date instead of a character and also have anumeric format of the date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Step 1: Format time as date \n",
    "# Format\n",
    "time <- as.Date(as.yearqtr(deposit_levels$time, format = \"%YQ%q\"))\n",
    "# Add the formatted date back into the dataset\n",
    "deposit_levels$time_as_date <- time\n",
    "\n",
    "# Step 2: Format time as numeric\n",
    "# Format\n",
    "time2 <- as.numeric(deposit_levels$time_as_date)\n",
    "#  Add the formatted date back into the dataset\n",
    "deposit_levels$time_as_numeric <- time2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "saveRDS(deposit_levels, \"deposits.rds\")  # Save the data to call it in other notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Step 3: Plot the model\n",
    "ggplot(deposit_levels, aes(x = time_as_date, y = deposits)) + \n",
    "  geom_point(color = \"blue\") +\n",
    "  labs(title = \"Deposit Levels from 1980 to 2024 (in $)\", \n",
    "       x = \"Time\", \n",
    "       y = \"Deposit Level (in $)\") +\n",
    "  theme_classic() + \n",
    "  theme(\n",
    "    plot.title = element_text(size = 16, face = \"bold\"), # Increase title size\n",
    "    axis.title.x = element_text(size = 12),             # Increase x-axis label size\n",
    "    axis.title.y = element_text(size = 12),             # Increase y-axis label size\n",
    "    axis.text = element_text(size = 12)                 # Increase axis tick labels size\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. Visualizing Linear Trend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Step 1: Time needs to be formatted as numeric for it to work ==> DONE above\n",
    "\n",
    "# Step 2: Fit the linear model\n",
    "linear_model <- lm(deposits ~ time_as_numeric ,data = deposit_levels) # Use time as numeric here\n",
    "\n",
    "# Step 3: Plot the model\n",
    "ggplot(deposit_levels, aes(x = time_as_date, y = deposits)) + \n",
    "  geom_point() +\n",
    "  geom_smooth(method = lm, col = \"green\", linewidth = 3) + \n",
    "  labs(title = \"Linear Fit of Deposit Levels (in $)\", \n",
    "       x = \"Time\", \n",
    "       y = \"Deposit Level (in $)\") +\n",
    "  theme_classic() + \n",
    "  theme(\n",
    "    plot.title = element_text(size = 20, face = \"bold\"), # Increase title size\n",
    "    axis.title.x = element_text(size = 14),             # Increase x-axis label size\n",
    "    axis.title.y = element_text(size = 14),             # Increase y-axis label size\n",
    "    axis.text = element_text(size = 12)                 # Increase axis tick labels size\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c. Visualizing Logistic Growth Trend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Not the same as logistic resgression\n",
    "\n",
    "- A mathematical model used to describe non-linear growth that slows as it approaches a saturation point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Step 1: Define a logistic function\n",
    "logistic_function <- function(x, L, k, x0) {\n",
    "  L / (1 + exp(-k * (x - x0)))  # L = carrying capacity, k = growth rate, x0 = midpoint\n",
    "}\n",
    "\n",
    "# Step 2: Fit the logistic model\n",
    "logistic_model <- nls(deposits ~ logistic_function(time_as_numeric, L, k, x0),\n",
    "                      data = deposit_levels,\n",
    "                      start = list(L = max(deposit_levels$deposits), k = 0.01, x0 = median(deposit_levels$time_as_numeric)))\n",
    "\n",
    "\n",
    "# Step 3: Add logistic trend predictions to the dataset\n",
    "deposit_levels <- deposit_levels %>%\n",
    "  mutate(logistic_trend = predict(logistic_model))\n",
    "\n",
    "# Step 4: Plot the linear model\n",
    "ggplot(deposit_levels, aes(x = time_as_date, y = deposits)) +\n",
    "  geom_point(color = \"black\", linewidth = 1) +  # Actual data\n",
    "  geom_line(aes(y = logistic_trend), color = \"red\", linewidth = 3) +  # Logistic trend\n",
    "  labs(title = \"Logistic Trend Fit of Deposit Levels (in $)\", \n",
    "       x = \"Date\", \n",
    "       y = \"Deposits (in $)\") +\n",
    "  theme_classic() +  # Simplify the appearance\n",
    "  theme(\n",
    "    plot.title = element_text(size = 18, face = \"bold\"), # Increase title size\n",
    "    axis.title.x = element_text(size = 14),             # Increase x-axis label size\n",
    "    axis.title.y = element_text(size = 14),             # Increase y-axis label size\n",
    "    axis.text = element_text(size = 12)                 # Increase tick label size\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d. Conclusion for Trend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why NOT logistic trend model?**\n",
    "\n",
    "- Even though, the logistic trend line seems to fit better past data, the logistic trend assumes market saturation and plateau. \n",
    "- But due to contextual information we have, we know that this is not a plateau but the slow growth was due to the 2008 crisis which is a one time event and not a recurring structural pattern.\n",
    "- If we pick logistic trend, it might lead to under-predictions if the future involves a period of growth.\n",
    "- This highlights the importance of blending data patterns with domain insights. A blind reliance on \"best fit\" could lead to a misinterpretation of the trends.\n",
    "\n",
    "**Why pick linear trend fit?**\n",
    "- Linear trends assume consistent growth over time, which aligns well with long-term economic expansion and deposit accumulation in a stable financial system\n",
    "- Plus, current high interest rate environment incentivizes deposits.\n",
    "- We see a continued economic recovery post 2008 crisis, suggesting deposits will grow, though slower than pre-crisis period.\n",
    "- Uncertainty about inflation and the economy may drive precautionary savings since we are also recovering from the covid 19 crisis.\n",
    "- Ultimately, choosing linear growth aligns with the economic rationale that deposits will likely grow as part of the recovery phase. \n",
    "\n",
    "**Additional Data Manipulation?**\n",
    "- Will we have to use log? Or will that change the linearity?\n",
    "- Applying a log transformation to your data can help make it fit better if your deposit levels grow exponentially rather than linearly. However, this doesn't change the fact that you're still modeling a linear trend—it just adjusts the scale to make it easier for Prophet to handle the data\n",
    "- If your data already behaves linearly, logging may unnecessarily distort it. ==> I think it's fine\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Code to check for linearit assumption\n",
    "linear_model1 <- lm(deposits ~ time_as_numeric, data = deposit_levels) # Linearity is struggling due to changepoints but I can add changepoints to prophet\n",
    "#plot(linear_model1)\n",
    "# Transform with log and see if it's better\n",
    "linear_model2 <- lm(log(deposits) ~ time_as_numeric, data = deposit_levels) # No it did not help improve\n",
    "#plot(linear_model2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Seasonality Expectation\n",
    "\n",
    "- Tools like Prophet and others can explicitly model seasonality if detected, which improves forecasting by capturing these periodic variations.\n",
    "\n",
    "- However it's important to manually double check and understand the context behind the model.\n",
    "\n",
    "- If seasonality exists but isn’t accounted for in the model, predictions may be inaccurate.\n",
    "\n",
    "- For instance, if deposits consistently increase in Q4 but your model doesn’t include seasonality, it will underestimate future deposits in that quarter.\n",
    "\n",
    "#### a. Let's plot a detrended ACF to check for seasonality\n",
    "\n",
    "**How to read an ACF Plot**\n",
    "\n",
    "- We look at the peaks that go above the blue line. Check lags based on the frequency:\n",
    "   - Lag = 12 for monthly data (12 months in a year)\n",
    "   - Lag = 4 for quarterly data\n",
    "   - Lag = 52 for weekly data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Step 1: Convert deposits into a time series object. ACF only takes in time series data. \n",
    "deposit_time_series <- ts(deposit_levels$deposits, \n",
    "                        start = c(1984, 1), # we could have put deposit_levels$time_as_numeric but later on when plotting randomness the x axis won't show time properly so we just hardcode to tell where X axis will start and with frequency it knows how to link X and Y. \n",
    "                        frequency = 4) # Quarterly frequency \n",
    "\n",
    "# Step 2: Subtract the trend component, since the trend dominates, remove it to isolate the seasonality\n",
    "        # Decompose the trend: breaks down the trend\n",
    "deposit_decompose_trend <- decompose(deposit_time_series, type=\"additive\") # breaks down dataset into 4 categories: observed data, trend, seasonality and randomness plot(deposit_decompose_trend) to see all 4 \n",
    "        # Remove the trend from the time series data\n",
    "deposit_detrended <- deposit_time_series - deposit_decompose_trend$trend\n",
    "\n",
    "# Step 3: Plot ACF to check for seasonality\n",
    "acf(deposit_detrended, na.action = na.omit, main = \"Autocorrelation Plot of the Detrended of Deposits\") # NAs comes from decomposition algorithm uses a moving window to calculate the trend, and it cannot compute values for the edges of the series."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. Conclusion for Seasonality\n",
    "\n",
    "- There are no peaks above the blue line at lag 4 (Quarterly Seasonality), the absence of such spikes suggests that quarterly seasonality is weak or not present in this case.\n",
    "\n",
    "- For now, we will let Prophet's built-in seasonal detection handle any weak seasonal patterns\n",
    "\n",
    "### Step 4: Outliers Expectation\n",
    "\n",
    "**Why check for outliers**:  \n",
    "\n",
    "- Prophet is designed to handle outliers and is fairly robust to their presence.\n",
    "\n",
    "- However, extreme anomalies (like sudden, sharp increases or drops) can still affect the trend and seasonal components, distorting predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Check steps here: https://sqlpad.io/tutorial/interquartile-range/#:~:text=The%20standard%20method%20involves%20calculating,these%20bounds%20are%20potential%20outliers.\n",
    "\n",
    "# Step 1: Calculate the IQR\n",
    "IQR_value <- IQR(deposit_levels$deposits)\n",
    "\n",
    "# Step 2: Define Boundaries\n",
    "lower_bound <- quantile(deposit_levels$deposits, 0.25) - 1.5 * IQR_value\n",
    "upper_bound <- quantile(deposit_levels$deposits, 0.75) + 1.5 * IQR_value\n",
    "\n",
    "# Step 3: Flag Outliers\n",
    "outliers <- subset(deposit_levels, deposits < lower_bound | deposits > upper_bound)\n",
    "\n",
    "# step 4: Print Outliers\n",
    "outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion for Outliers:**\n",
    "\n",
    "- Based of the results, it looks like the IQR method did not flag any outliers in the data. The deposit levels in the dataset fall within the acceptable range defined by the IQR method. The deposit levels may be naturally smooth without extreme spikes or dips (maybe lol?)\n",
    "\n",
    "- Even without detected outliers, we should remain aware of contextual anomalies (e.g., the 2008 crisis) that could still affect model interpretation and trends.\n",
    "\n",
    "### Step 5: Noise\n",
    "\n",
    "\n",
    "**Why check for noise?**\n",
    "\n",
    "- Noise refers to the unexplained or irrelevant variation in the data that cannot be attributed to meaningful patterns.\n",
    "- Prophet is robust to noise, but excessive noise may require preprocessing (e.g., smoothing).\n",
    "- A higher proportion (close to 1) indicates the data is dominated by noise.\n",
    "- A lower proportion (closer to 0) means the data has more structured patterns (e.g., trend, seasonality)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Calculate the variance of the residuals to quantify the noise and Compare this variance to the overall variance in the observed data.\n",
    "\n",
    "# Variance of residuals (randomness)\n",
    "residual_variance <- var(deposit_decompose_trend$random, na.rm = TRUE) # Use our decomposed dataset\n",
    "\n",
    "# Variance of the original data\n",
    "observed_variance <- var(deposit_time_series, na.rm = TRUE)  # Use our dataset that was transformed to time series, if we use deposit_levels directly it won't work\n",
    "\n",
    "# Proportion of variance due to noise\n",
    "global_noise_proportion <- residual_variance / observed_variance\n",
    "global_noise_proportion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion for Noise**\n",
    "\n",
    "- Our noise proportion of approximately 0.00067 (a very small number), which means: noise contributes very little to the overall variability in your data. \n",
    "- This is typical seen when our data has a strong long-term trend.\n",
    "- So there won't be any need for excessive smoothing or denoising steps. Prophet can effectively handle the low-level noise.\n",
    "\n",
    "#### Step 6: Localised Randomness\n",
    "\n",
    "**Why check for randomness?**\n",
    "\n",
    "-  Randomness, as shown in decomposition, includes localized irregularities or residual variations that aren't explained by the trend or seasonality but may include event-driven or one-off effects (like the 2008 crisis). It’s more localized and descriptive of specific periods rather than a global property like noise.\n",
    "- While Prophet is robust to some noise, high randomness (localized irregularities) can make it harder to discern meaningful patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# plot(deposit_decompose_trend) ==> from the ACF code above in step 3.a\n",
    "\n",
    "plot(deposit_decompose_trend$random,\n",
    "    xlab = \"Time\",\n",
    "    ylab = \"Random Component\",\n",
    "    main = \"Random Component of Deposits over Time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion for Randomness**\n",
    "- There is significant randomness during the 2008 crisis period.\n",
    "- We can add on a changepoint to handle abrupt trend changes.\n",
    "- Prophet will automatically detects changepoints in the trend. However, you can manually specify changepoints (e.g., around 2008) where the data exhibits high variability.\n",
    "\n",
    "## Part 3: Forecasting with Prophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "prophet_data_1 <- deposit_levels[, c(\"deposits\", \"time_as_date\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Prepared the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Step 1: Prepare data for prophet to work\n",
    "\n",
    "# you have to rename them as ds and y for prophet to work and it will automatically ignore all other columns ==> RULE\n",
    "prophet_data_1$ds <- as.Date(prophet_data_1$time_as_date)\n",
    "prophet_data_1$y <- prophet_data_1$deposits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Fit Prophet on training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Step 1: Initialize the model\n",
    "prophet_model<- prophet(changepoints = c(\"2008-01-01\")) # Add Customizations based on assumptions made above https://stackoverflow.com/questions/64822488/how-to-use-prophets-make-future-dataframe-with-multiple-regressors\n",
    "# We can later add on additional regressor to the best model\n",
    "\n",
    "# Step 2: Fit the model: Training the model\n",
    "prophet_model <- fit.prophet(prophet_model, prophet_data_1) # syntax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Create a Future Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "future_data <- make_future_dataframe(prophet_model, periods = 33, freq = \"quarter\") # start counting from the end of the training data: 2020 Q4 to 2028 Q1\n",
    "#future_data # The output of make_future_dataframe() will include both the training data dates and the newly created future dates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Predict Future Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "prediction_data <- predict(prophet_model, future_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Visualize the Forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Ensure 'ds' is a Date object\n",
    "prophet_data_1$ds <- as.Date(prophet_data_1$ds)\n",
    "prediction_data$ds <- as.Date(prediction_data$ds)\n",
    "\n",
    "ggplot() +\n",
    "  # Add historical data points\n",
    "  geom_point(data = prophet_data_1, aes(x = ds, y = y, color = \"Historical Data\"), size = 1.5, alpha = 0.8) +\n",
    "  # Add the predicted trend line\n",
    "  geom_line(data = prediction_data, aes(x = ds, y = yhat, color = \"Predicted Trend\"), size = 2) +\n",
    "  # Add uncertainty intervals for predictions\n",
    "  geom_ribbon(\n",
    "    data = prediction_data,\n",
    "    aes(x = ds, ymin = yhat_lower, ymax = yhat_upper, fill = \"Uncertainty Interval\"),\n",
    "    alpha = 0.2\n",
    "  ) +\n",
    "  # Add labels and a title\n",
    "  labs(\n",
    "    title = \"Historical Data and Prediction with Prophet\",\n",
    "    x = \"Date\",\n",
    "    y = \"Deposits (in $)\",\n",
    "    color = \"Legend\",\n",
    "    fill = \"Legend\"\n",
    "  ) +\n",
    "  # Customize colors for the legend\n",
    "  scale_color_manual(\n",
    "    values = c(\"Historical Data\" = \"black\", \"Predicted Trend\" = \"red\")\n",
    "  ) +\n",
    "  scale_fill_manual(\n",
    "    values = c(\"Uncertainty Interval\" = \"blue\")\n",
    "  ) +\n",
    "  # Add a minimal theme\n",
    "  theme_minimal() +\n",
    "  theme_classic() +\n",
    "  theme(\n",
    "    plot.title = element_text(hjust = 0.5, size = 14, face = \"bold\"),\n",
    "    axis.title.x = element_text(size = 12),\n",
    "    axis.title.y = element_text(size = 12),\n",
    "    legend.title = element_text(size = 12, face = \"bold\"),\n",
    "    legend.text = element_text(size = 10)\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "prophet_plot_components(prophet_model, prediction_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Evaluation of the Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Step 1: Split Data between training and test data\n",
    "train_data <- subset(prophet_data_1, ds <= \"2019-12-31\") # Training data: 1984 Q1 to 2020 Q4\n",
    "test_data <- subset(prophet_data_1, ds > \"2019-12-31\")  # Testing data: 2021 Q1 to 2024 Q2\n",
    "\n",
    "# Step 2: Prepare data for prophet to work\n",
    "# you have to rename them as ds and y for prophet to work and it will automatically ignore all other columns ==> RULE\n",
    "train_data$ds <- as.Date(train_data$ds)\n",
    "train_data$y <- train_data$y\n",
    "\n",
    "test_data$ds <- as.Date(test_data$ds)\n",
    "test_data$y <- test_data$y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Compare predictions with test data \n",
    "\n",
    "- The test data is unseen data (not used in training) that allows you to assess how well your model generalizes to new data.\n",
    "- Comparing predictions to actual values in the test data helps identify whether the model overfits or underfits.\n",
    "- A good forecast should perform well not only on the training data but also on the test data. This is a crucial check before relying on the model for real-world forecasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Step 1: Create future dataframe for test data\n",
    "future_test_data <- make_future_dataframe(prophet_model, periods = nrow(test_data), freq = \"quarter\") # Testing data: 2021 Q1 to 2024 Q2\n",
    "#future_test_data\n",
    "\n",
    "# Step 2: Make predictions for the test data\n",
    "prediction_test_data <- predict(prophet_model, future_test_data) \n",
    "prediction_test_data$ds <- as.Date(prediction_test_data$ds) #make sure ds is in date format or else merge will fail\n",
    "\n",
    "# Step 3: Merge predictions with actual test data: \n",
    "# The merging step combines your actual test data with the predictions generated by Prophet. This creates a single dataframe where you can directly compare the actual (y) and predicted (yhat) values for each time point (ds)\n",
    "prediction_test_results <- merge(test_data, prediction_test_data[, c(\"ds\", \"yhat\")], by = \"ds\")\n",
    "\n",
    "# Step 4: Plot the predictions vs actuals\n",
    "ggplot(prediction_test_results, aes(x = ds)) +\n",
    "  geom_line(aes(y = y, color = \"Actual\"), size = 1) + # Actual test data\n",
    "  geom_line(aes(y = yhat, color = \"Predicted\"), size = 1, linetype = \"dashed\") + # Predictions\n",
    "  labs(\n",
    "    title = \"Predictions vs Actuals on Test Data with Prophet Model\",\n",
    "    x = \"Time\",\n",
    "    y = \"Deposit Levels\",\n",
    "    color = \"Legend\"\n",
    "  ) +\n",
    "  theme_classic()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations**\n",
    "- The predicted model (blue dashed line) done with Prophet does not closely align with the test (actual) data from 2021 to 2024. \n",
    "- There's a significant divergence between the actuals and predictions: the model overpredicts significantly in this range, showing an upward trend, whereas the actual values flatten or slightly decrease."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2. Compute Perfomance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Mean Absolute Error (MAE)\n",
    "\n",
    "actual <- prediction_test_results$y\n",
    "predicted <- prediction_test_results$yhat\n",
    "\n",
    "MAE_prophet <- mean(abs(actual - predicted))\n",
    "MAE_prophet\n",
    "\n",
    "# Root Mean Squared Error (RMSE)\n",
    "RMSE_prophet <- sqrt(mean((actual - predicted)^2))\n",
    "RMSE_prophet\n",
    "\n",
    "# Mean Absolute Percentage Error (MAPE)\n",
    "MAPE_prophet <- mean(abs(actual - predicted) / actual) * 100\n",
    "MAPE_prophet\n",
    "\n",
    "# Compute lagged actual values\n",
    "lagged_actual <- c(NA, head(actual, -1))\n",
    "\n",
    "# Calculate MAE of naive forecast (excluding first NA value)\n",
    "MAE_naive <- mean(abs(actual[-1] - lagged_actual[-1]), na.rm = TRUE)\n",
    "\n",
    "# Calculate MASE\n",
    "MASE_prophet <- MAE_prophet / MAE_naive\n",
    "MASE_prophet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 5: Adding on External Regressors to the Model\n",
    "\n",
    "## Step 1: Select Relevant Features through domain knowledge\n",
    "\n",
    "- Our goal here is research features that are likely to affect deposit level\n",
    "- To narrow our options we will use a mix of domain knowledge and a LASSO regression to undertand the relationship between deposit and those potential regressors\n",
    "\n",
    "> Here is a list of features through reserach and domain knowledge that I believe most impact deposit levels:\n",
    "- GDP Growth Rate: Stronger economic growth can lead to higher income and savings\n",
    "- Unemployment Rate: Higher unemployment can reduce deposits due to lower income\n",
    "- Consumer Confidence Index: Reflects consumers' willingness to save vs. spend\n",
    "- Population Growth: Larger populations in a region can increase deposit levels over time\n",
    "- 10 Year Treasury Yield: Represent the interest rate environment, directly affecting deposit rates and competing as an alternative safe investment option.\n",
    "- Federal Funds Effective Rate: impacts savings account interest rate\n",
    "- Disposable Personal Income (percent change from year ago): ok\n",
    "- Consumer Price Index (units: growth rate previos period in decimal): measure of inflation rate. Higher inflation may reduce disposable income and savings levels\n",
    "\n",
    "## Step 2: Narrow Down Features through LASSO Regression\n",
    "\n",
    "### A. Prepare the Data\n",
    "\n",
    "- Combine deposit level data and regressors data into one => deposit_regressor_data\n",
    "- Keeping the target variable (Deposit_Levels) untouched.\n",
    "- Ensuring non-regressor columns (e.g., time) are excluded from scaling. time: A timestamp or date column (used for splitting but not for scaling or modeling).\n",
    "- Scaling only the features (regressors). Other columns (GDP_Growth, Unemployment_Rate, Interest_Rate, SP500, etc.): Features to be scaled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Combine deposit level data and regressors data into one => deposit_regressor_data\n",
    "deposit_regressor_data <- merge(deposit_levels, regressors_data, by = c(\"time_as_date\", \"time_as_numeric\"))\n",
    "\n",
    "# Pre process only the regressors inside your data\n",
    "\n",
    "# Exclude all that's not a regressor like the target varaiable (deposit) and time variables\n",
    "regressors_col <- setdiff(names(deposit_regressor_data), c(\"logistic_trend\", \"time\", \"time_as_date\", \"time_as_numeric\", \"deposits\"))\n",
    "\n",
    "# The preProcess function centers (mean = 0) and scales (std = 1) your predictors => Apply scaling to the feature columns\n",
    "pre_process_data <- preProcess(deposit_regressor_data[, regressors_col], method=c(\"center\", \"scale\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Split dataset into Training dataset and Test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Split data into training and testing based on time\n",
    "train_data_regressor <- subset(deposit_regressor_data, time_as_date <= \"2019-12-31\") # Training data: 1984 Q1 to 2020 Q4\n",
    "test_data_regressor <- subset(deposit_regressor_data, time_as_date > \"2019-12-31\")  # Testing data: 2021 Q1 to 2024 Q2\n",
    "\n",
    "# Apply the pre-processing to our training and testing dataset\n",
    "\n",
    "# Scale training data (apply preprocessing to the regressor columns only)\n",
    "train_data_scaled <- train_data_regressor\n",
    "train_data_scaled[, regressors_col] <- predict(pre_process_data, train_data_regressor[, regressors_col])\n",
    "\n",
    "# Scale testing data using the same preprocessing parameters\n",
    "test_data_scaled <- test_data_regressor\n",
    "test_data_scaled[, regressors_col] <- predict(pre_process_data, test_data_regressor[, regressors_col])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Fit the LASSO Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Create matrices for modeling for training data\n",
    "\n",
    "X_train_lasso <- as.matrix(train_data_scaled[, regressors_col])\n",
    "X_train_lasso[is.na(X_train_lasso)] <- 0 # fill the mssing value with 0\n",
    "y_train_lasso <- train_data_scaled$deposits\n",
    "\n",
    "# Create matrices for modeling for testing data\n",
    "X_test_lasso <- as.matrix(test_data_scaled[, regressors_col])\n",
    "y_test_lasso <- test_data_scaled$deposits\n",
    "\n",
    "# Fit the lasso_model\n",
    "lasso_model <- cv.glmnet(X_train_lasso, y_train_lasso, alpha = 1)\n",
    "\n",
    "# Extract selected features at lambda.min\n",
    "selected_features <- coef(lasso_model, s = \"lambda.min\")\n",
    "print(selected_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You’ve chosen not to scale the deposits column, which is fine as long as you don’t intend to interpret the coefficients from the LASSO model directly. However, if coefficient interpretation is needed, it’s often better to scale the target variable as well (standardize deposits).\n",
    "-  Lasso revealed that only unemployment rate and GDP are useful here.\n",
    "\n",
    "# Part 6: Add the regression into our model\n",
    "\n",
    "## Step 1: Plugging Regressors into Prophet\n",
    "\n",
    "- With a MAPE of 10%, it’s not ideal, but it’s not a dealbreaker\n",
    "- Even if the Lasso model's MAPE is high, the regressors may still contain useful signals that Prophet can leverage.\n",
    "- Prophet doesn't rely solely on the regressors—it also models time-based trends and seasonality, which can compensate for some of the inaccuracy in the regressors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "#Step 1: create dataframe\n",
    "\n",
    "prophet_data_2 <- data.frame(\n",
    "    ds = deposit_regressor_data$time_as_date,\n",
    "    y = deposit_regressor_data$deposits,\n",
    "    GDP = deposit_regressor_data$GDP,\n",
    "    unemployment_rate = deposit_regressor_data$unemployment_rate,\n",
    "    fed_rate = deposit_regressor_data$fed_rate,\n",
    "    ConsumerPriceIndex = deposit_regressor_data$ConsumerPriceIndex # stop here if it makes things worse\n",
    "    )\n",
    "\n",
    "# Step 2: Handle Consumer price index last missing value\n",
    "prophet_data_2$ConsumerPriceIndex[is.na(prophet_data_2$ConsumerPriceIndex)] <- mean(prophet_data_2$ConsumerPriceIndex, na.rm = TRUE)\n",
    "\n",
    "# Step 3: Initialize the model by adding on regressors and Fit Prophet\n",
    "\n",
    "# Initialize the model\n",
    "prophet_model_2 <- prophet()\n",
    "\n",
    "# Add regressors\n",
    "prophet_model_2 <- add_regressor(prophet_model_2, 'GDP')\n",
    "prophet_model_2 <- add_regressor(prophet_model_2, 'unemployment_rate')\n",
    "prophet_model_2 <- add_regressor(prophet_model_2, 'ConsumerPriceIndex')\n",
    "prophet_model_2 <- add_regressor(prophet_model_2, 'fed_rate')\n",
    "\n",
    "# Step 4: Fit the model: full data the model\n",
    "prophet_model_2 <- fit.prophet(prophet_model_2, prophet_data_2) # syntax\n",
    "\n",
    "# Step 5: Create a future dataframe\n",
    "future_data_2 <- make_future_dataframe(prophet_model_2, periods = 15, freq = \"quarter\") # start counting from the end of the training data: 2020 Q4 to 2028 Q1\n",
    "#future_data_2  # The output of make_future_dataframe() will include both the training data dates and the newly created future dates.future_train_data_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Step 1: Create dataframes for GDP and unemployment rate\n",
    "gdp_data <- data.frame(\n",
    "  ds = prophet_data_2$ds, # Use the same date column as in your training data\n",
    "  y = prophet_data_2$GDP  # Use the GDP column\n",
    ")\n",
    "\n",
    "unemployment_data <- data.frame(\n",
    "  ds = prophet_data_2$ds, # Use the same date column as in your training data\n",
    "  y = prophet_data_2$unemployment_rate # Use the unemployment rate column\n",
    ")\n",
    "\n",
    "fed_data <- data.frame(\n",
    "  ds = prophet_data_2$ds, # Use the same date column as in your training data\n",
    "  y = prophet_data_2$fed_rate # Use the fed rate column\n",
    ")\n",
    "\n",
    "CPI_data <- data.frame(\n",
    "  ds = prophet_data_2$ds, # Use the same date column as in your training data\n",
    "  y = prophet_data_2$ConsumerPriceIndex # Use the CPI column\n",
    ") \n",
    "\n",
    "# Step 2: Fit Prophet models for GDP and unemployment rate\n",
    "gdp_model <- prophet(gdp_data) # Fit the GDP model\n",
    "unemployment_model <- prophet(unemployment_data) # Fit the unemployment rate model\n",
    "fed_model <- prophet(fed_data)\n",
    "CPI_model <- prophet(CPI_data)\n",
    "\n",
    "# Step 3: Generate future dataframes matching the number of rows in future_data_2\n",
    "future_gdp <- make_future_dataframe(gdp_model, periods = nrow(future_data_2) - nrow(prophet_data_2), freq = \"quarter\")\n",
    "future_unemployment <- make_future_dataframe(unemployment_model, periods = nrow(future_data_2) - nrow(prophet_data_2), freq = \"quarter\")\n",
    "future_fed <- make_future_dataframe(fed_model, periods = nrow(future_data_2) - nrow(prophet_data_2), freq = \"quarter\")\n",
    "future_CPI <- make_future_dataframe(CPI_model, periods = nrow(future_data_2) - nrow(prophet_data_2), freq = \"quarter\")\n",
    "\n",
    "# Step 4: Forecast the future regressor values\n",
    "gdp_forecast <- predict(gdp_model, future_gdp)\n",
    "unemployment_forecast <- predict(unemployment_model, future_unemployment)\n",
    "fed_forecast <- predict(fed_model, future_fed)\n",
    "CPI_forcast <- predict(CPI_model, future_CPI)\n",
    "\n",
    "# Step 5: Extract forecasted values for GDP and unemployment rate\n",
    "future_GDP_values <- gdp_forecast$yhat[(nrow(prophet_data_2) + 1):nrow(future_gdp)]\n",
    "future_unemployment_values <- unemployment_forecast$yhat[(nrow(prophet_data_2) + 1):nrow(future_unemployment)]\n",
    "future_fed_rate_values <- fed_forecast$yhat[(nrow(prophet_data_2) + 1):nrow(future_fed)]\n",
    "future_CPI_values <- CPI_forcast$yhat[(nrow(prophet_data_2) + 1):nrow(future_CPI)]\n",
    "\n",
    "# Step 6: Add future GDP and unemployment rate values to future_data_2\n",
    "future_data_2$GDP <- c(prophet_data_2$GDP, future_GDP_values) # Combine training and future GDP values\n",
    "future_data_2$unemployment_rate <- c(prophet_data_2$unemployment_rate, future_unemployment_values) # Combine training and future unemployment rate values\n",
    "future_data_2$fed_rate <- c(prophet_data_2$fed_rate, future_fed_rate_values) \n",
    "future_data_2$ConsumerPriceIndex <- c(prophet_data_2$ConsumerPriceIndex, future_CPI_values) \n",
    "\n",
    "# Step 7: Verify the updated future_data_2\n",
    "\n",
    "head(future_data_2) # Check the final dataframe with added co\n",
    "tail(future_data_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Step 8: Predict Future Values\n",
    "prediction_data_2 <- predict(prophet_model_2, future_data_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "saveRDS(prediction_data_2, \"prediction_data_2.rds\")  # Save the data to call it in other notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "prophet_plot_components(prophet_model_2, prediction_data_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Visualize the foreacast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Ensure 'ds' is a Date object\n",
    "prophet_data_2$ds <- as.Date(prophet_data_2$ds)\n",
    "prediction_data_2$ds <- as.Date(prediction_data_2$ds)\n",
    "\n",
    "ggplot() +\n",
    "  # Add historical data points\n",
    "  geom_point(data = prophet_data_2, aes(x = ds, y = y, color = \"Historical Data\"), size = 1.5, alpha = 0.8) +\n",
    "  # Add the predicted trend line\n",
    "  geom_line(data = prediction_data_2, aes(x = ds, y = yhat, color = \"Predicted Trend\"), size = 2) +\n",
    "  # Add uncertainty intervals for predictions\n",
    "  geom_ribbon(\n",
    "    data = prediction_data_2,\n",
    "    aes(x = ds, ymin = yhat_lower, ymax = yhat_upper, fill = \"Uncertainty Interval\"),\n",
    "    alpha = 0.2\n",
    "  ) +\n",
    "  # Add labels and a title\n",
    "  labs(\n",
    "    title = \"Historical Data and Prediction with Prophet\",\n",
    "    x = \"Date\",\n",
    "    y = \"Deposits (in $)\",\n",
    "    color = \"Legend\",\n",
    "    fill = \"Legend\"\n",
    "  ) +\n",
    "  # Customize colors for the legend\n",
    "  scale_color_manual(\n",
    "    values = c(\"Historical Data\" = \"black\", \"Predicted Trend\" = \"red\")\n",
    "  ) +\n",
    "  scale_fill_manual(\n",
    "    values = c(\"Uncertainty Interval\" = \"blue\")\n",
    "  ) +\n",
    "  # Add a minimal theme\n",
    "  theme_classic() +\n",
    "  theme(\n",
    "    plot.title = element_text(hjust = 0.5, size = 14, face = \"bold\"),\n",
    "    axis.title.x = element_text(size = 12),\n",
    "    axis.title.y = element_text(size = 12),\n",
    "    legend.title = element_text(size = 12, face = \"bold\"),\n",
    "    legend.text = element_text(size = 9)\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Evaluation of the Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Step 1: Split Data between training and test data\n",
    "train_data_2 <- subset(prophet_data_2, ds <= \"2019-12-31\") # Training data: 1984 Q1 to 2020 Q4\n",
    "test_data_2 <- subset(prophet_data_2, ds > \"2019-12-31\")  # Testing data: 2021 Q1 to 2024 Q2\n",
    "\n",
    "# Step 2: Prepare data for prophet to work\n",
    "# you have to rename them as ds and y for prophet to work and it will automatically ignore all other columns ==> RULE\n",
    "train_data_2$ds <- as.Date(train_data_2$ds)\n",
    "train_data_2$y <- train_data_2$y\n",
    "\n",
    "test_data_2$ds <- as.Date(test_data_2$ds)\n",
    "test_data_2$y <- test_data_2$y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Step 3: Initialize the model by adding on regressors and Fit Prophet\n",
    "\n",
    "# Initialize the model\n",
    "prophet_model_test <- prophet()\n",
    "\n",
    "# Add regressors\n",
    "prophet_model_test <- add_regressor(prophet_model_test, 'GDP')\n",
    "prophet_model_test <- add_regressor(prophet_model_test, 'unemployment_rate')\n",
    "prophet_model_test <- add_regressor(prophet_model_test, 'ConsumerPriceIndex')\n",
    "prophet_model_test <- add_regressor(prophet_model_test, 'fed_rate')\n",
    "\n",
    "# Step 4: Fit the model: Training the model\n",
    "prophet_model_test <- fit.prophet(prophet_model_test, train_data_2) # syntax\n",
    "\n",
    "# Step 5: \n",
    "future_test_data_2 <- make_future_dataframe(prophet_model_test, periods = nrow(test_data_2), freq = 'quarter')\n",
    "\n",
    "# Step 6: Train models for GDP and unemployment rate, CPI and fed rate\n",
    "# You do not need to forcast them since here you alraedy have the values for the test dataframe\n",
    "future_test_data_2$GDP <- test_data_2$GDP\n",
    "future_test_data_2$unemployment_rate <- test_data_2$unemployment_rate\n",
    "future_test_data_2$ConsumerPriceIndex <- test_data_2$ConsumerPriceIndex\n",
    "future_test_data_2$fed_rate <- test_data_2$fed_rate\n",
    "\n",
    "# Step 7: Predict for the test period\n",
    "forecast_test <- predict(prophet_model_test, future_test_data_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Merge predictions with actual test data: \n",
    "\n",
    "forecast_test$ds <- as.Date(forecast_test$ds) #make sure ds is in date format or else merge will fail\n",
    "test_data_2$ds <- as.Date(test_data_2$ds)\n",
    "\n",
    "# The merging step combines your actual test data with the predictions generated by Prophet. This creates a single dataframe where you can directly compare the actual (y) and predicted (yhat) values for each time point (ds)\n",
    "forecast_test_results <- merge(test_data_2, forecast_test[, c(\"ds\", \"yhat\")], by = \"ds\")\n",
    "tail(forecast_test_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Calculate naive forecast\n",
    "naive_forecast <- c(NA, head(test_data_2$y, -1))  # Shift y_test by one step for naive forecast\n",
    "\n",
    "# Create a data frame for plotting\n",
    "naive_vs_model <- data.frame(\n",
    "  Date = test_data_2$ds,  # Dates from test data\n",
    "  Actual_Deposits = test_data_2$y,  # Actual deposit values\n",
    "  Naive_Forecast = naive_forecast,  # Naive forecast values\n",
    "  Predicted_Deposits = forecast_test_results$yhat  # Predicted deposit values\n",
    ")\n",
    "\n",
    "# Plot the data\n",
    "library(ggplot2)\n",
    "ggplot(naive_vs_model, aes(x = Date)) +\n",
    "  geom_line(aes(y = Actual_Deposits, color = \"Actual Deposits\"), size = 2) +  # Line for actual deposits\n",
    "  geom_line(aes(y = Naive_Forecast, color = \"Naive Forecast\"), size = 2, linetype = \"dotted\") +  # Dotted line for naive forecast\n",
    "  geom_line(aes(y = Predicted_Deposits, color = \"Predicted Deposits\"), size = 2, linetype = \"dashed\") +  # Dashed line for model predictions\n",
    "  labs(\n",
    "    title = \"Actual vs Naive vs Predicted Deposits (2020-2024)\",\n",
    "    x = \"Date\",\n",
    "    y = \"Deposits (in $)\",\n",
    "    color = \"Legend\"\n",
    "  ) +\n",
    "  theme_minimal() +\n",
    "  theme(\n",
    "    plot.title = element_text(hjust = 0.5, size = 14, face = \"bold\"),\n",
    "    axis.title = element_text(size = 12),\n",
    "    legend.title = element_text(size = 12, face = \"bold\"),\n",
    "    legend.text = element_text(size = 10)\n",
    "  )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "name": "R"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
